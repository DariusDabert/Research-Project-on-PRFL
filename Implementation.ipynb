{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creation du Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# Original transform\n",
    "original_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download and load the original dataset\n",
    "original_trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=original_transform)\n",
    "\n",
    "# Function to create rotated dataset\n",
    "def create_rotated_dataset(dataset, rotation_angle):\n",
    "    rotated_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.5,), (0.5,)),\n",
    "                                            transforms.RandomRotation(rotation_angle)])\n",
    "    rotated_dataset = copy.deepcopy(dataset)\n",
    "    rotated_dataset.transform = rotated_transform\n",
    "    return rotated_dataset\n",
    "\n",
    "# Function to create private label dataset\n",
    "def create_private_label_dataset(dataset, cluster_index):\n",
    "    private_label_transform = transforms.Lambda(lambda y: (y + cluster_index) % 10)\n",
    "    private_label_dataset = copy.deepcopy(dataset)\n",
    "    private_label_dataset.targets = private_label_transform(private_label_dataset.targets)\n",
    "    return private_label_dataset\n",
    "\n",
    "# Create rotated datasets for each cluster\n",
    "rotated_datasets = [create_rotated_dataset(original_trainset, k * 90) for k in range(4)]\n",
    "\n",
    "# Create private label datasets for each cluster\n",
    "datasets = [create_private_label_dataset(rotated_datasets[k], k) for k in range(4)]\n",
    "\n",
    "# Split each dataset into clusters\n",
    "cluster_size = len(original_trainset) // 5\n",
    "\n",
    "clustered_datasets = [Subset(datasets[k], range(i * cluster_size, (i + 1) * cluster_size)) for k in range(4) for i in range(5)]\n",
    "\n",
    "# Create dataloaders for each cluster\n",
    "# rotated_dataloaders = [DataLoader(dataset, batch_size=64, shuffle=True) for dataset in clustered_rotated_datasets]\n",
    "dataloaders = [DataLoader(dataset, batch_size=64, shuffle=True) for dataset in clustered_datasets]\n",
    "print(len(dataloaders))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ALGORITHME DU PAPIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, (5,5), padding=2)\n",
    "        self.pool1 = nn.MaxPool2d((2,2))\n",
    "        self.conv2 = nn.Conv2d(4, 8, (5,5), padding=2)\n",
    "        self.pool2 = nn.MaxPool2d((2,2))\n",
    "        self.conv3 = nn.Conv2d(8, 16, (5,5), padding=2)\n",
    "        self.fc = nn.Linear(16*7*7, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,1,28,28)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "models = {}\n",
    "\n",
    "for i in range(20):\n",
    "    models[f'client{i}'] = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THRESHOLD CLUSTERING\n",
    "\n",
    "def threshold_clustering(Z, K, initial_centers):\n",
    "    \"\"\"\n",
    "    Perform Threshold-Clustering on a set of points using PyTorch.\n",
    "\n",
    "    Parameters:\n",
    "    Z (List): list of moments to be clustered, shape (N, d)\n",
    "    K (int): Number of clusters\n",
    "    initial_centers (Tensor): Initial cluster centers, shape (K, d)\n",
    "    tau (Tensor): Radius for each cluster in each round, shape (K, M)\n",
    "\n",
    "    Returns:\n",
    "    Tensor: Final cluster centers, shape (K, d)\n",
    "    \"\"\"\n",
    "\n",
    "    centers = [center.clone() for center in initial_centers]\n",
    "    tau = np.zeros(K)\n",
    "\n",
    "    for k in range(K):\n",
    "        distances = torch.stack([(z - centers[k]).norm(dim=0) for z in Z])\n",
    "        # tau[k] is the distance to th\n",
    "        # e Kth closest point\n",
    "        tau[k] = torch.topk(distances, len(Z) // K)[0][-1]\n",
    "        within_radius = distances <= tau[k]\n",
    "        centers[k] = torch.mean(torch.stack([z * within_radius[i] + centers[k] * ~within_radius[i] for i, z in enumerate(Z)]), dim=0)\n",
    "\n",
    "    # Find the closest center for each point\n",
    "    closest_centers = torch.stack([torch.stack([(z - center).norm(dim=0) for z in Z]) for center in centers])\n",
    "    closest_center_indices = torch.argmin(closest_centers, dim=0)\n",
    "    return centers, closest_center_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(dataloaders, models, epoch=1, batch_size=16, rate=1e-3, momentum=0.9):\n",
    "\n",
    "    optimizers = [torch.optim.SGD(models[f'client{i}'].parameters(), lr=rate, momentum = 0.9) for i in range(20)]\n",
    "\n",
    "    losses_fn = [nn.CrossEntropyLoss()  for i in range(20)]\n",
    "\n",
    "    moments = []\n",
    "\n",
    "    losses = {}\n",
    "\n",
    "    for items in zip(*dataloaders):\n",
    "            \n",
    "            for i, item in enumerate(items):\n",
    "                optimizers[i].zero_grad()\n",
    "                sample, target = item\n",
    "                # compute loss and grad\n",
    "                losses[f'client{i}'] = losses_fn[i](models[f'client{i}'](sample), target)\n",
    "                losses[f'client{i}'].backward()\n",
    "                #compute grad\n",
    "\n",
    "                grad = torch.cat([param.grad.view(-1) if param.grad is not None else torch.zeros_like(param).view(-1) for param in models[f'client{i}'].parameters()])\n",
    "\n",
    "                moments.append(grad)\n",
    "\n",
    "            break\n",
    "    \n",
    "    center_clusters = [moments[i] for i in np.random.choice(20, 4, replace=False)]\n",
    "\n",
    "    for t in range(epoch):\n",
    "        compt = 0\n",
    "        for items in tqdm(zip(*dataloaders)):\n",
    "            for i, item in enumerate(items):\n",
    "                optimizers[i].zero_grad()\n",
    "                sample, target = item\n",
    "\n",
    "                losses[f'client{i}'] = losses_fn[i](models[f'client{i}'](sample), target)\n",
    "\n",
    "                grad = torch.cat([param.grad.view(-1) if param.grad is not None else torch.zeros_like(param).view(-1) for param in models[f'client{i}'].parameters()])\n",
    "\n",
    "                moments[i] = momentum * grad + (1 - momentum) * moments[i]\n",
    "\n",
    "            if (compt%10 == 0) : \n",
    "                center_clusters, clustering = threshold_clustering(moments, 4, center_clusters)\n",
    "\n",
    "            for i in range(20):\n",
    "\n",
    "                losses[f'client{i}'].backward()\n",
    "                k = 0\n",
    "                # set the grad to be the same as in center_clusters[clustering[i]]\n",
    "                for param in models[f'client{i}'].parameters():\n",
    "                    param.grad = center_clusters[clustering[i]][k : k + param.grad.numel()].view(param.grad.shape)\n",
    "                    k += param.grad.numel()\n",
    "                \n",
    "                optimizers[i].step()\n",
    "\n",
    "            compt += 1\n",
    "\n",
    "            if compt == 100:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [01:07,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer(dataloaders, models, epoch=1, batch_size=16, rate=1e-3, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
